{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d44aea-08e6-40a6-b42d-87c17902a1d2",
   "metadata": {},
   "source": [
    "## PyTorch Model\n",
    "\n",
    "In this project we will go through the following models:\n",
    "\n",
    "- [Tensor Basics - Create, operation, GPU support](#tensors)\n",
    "- [Autograd - Linear regression model](#autograd)\n",
    "- [Training Loop with: Model, Loss, and Optimizer](#training)\n",
    "- [Neural Network - Datasets, DataLoader, Transforms, and Evaluation](#fneuron)\n",
    "- Convolutional Neural Network - save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaff0196-a706-47e8-a7e3-1fe57cfeeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first the imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa96b80-5d7a-481d-9b45-d0e2dd7734f4",
   "metadata": {},
   "source": [
    "<a id='tensors'></a>\n",
    "### Tensor Basics\n",
    "\n",
    "In PyTorch everything is based on tensors, which are multi-dimensional matrices containing the elements of a single data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f502538f-eaf2-4b0d-86da-68605a065dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## empty matrices\n",
    "## which basically grabs random blocks of memory\n",
    "## which we can create scalar\n",
    "sca = torch.empty(1) \n",
    "## or vectors\n",
    "vec = torch.empty(5)\n",
    "## or matrices\n",
    "matr = torch.empty(3, 3)\n",
    "## or multi-dimentional tensors\n",
    "mult = torch.empty(3, 3, 3)\n",
    "## we can aslo create random matrices\n",
    "## very similar to numpy\n",
    "random = torch.rand(2, 3, 4)\n",
    "## or zeros\n",
    "zeros = torch.zeros(3, 4, 5)\n",
    "## or ones\n",
    "ones = torch.ones(2, 2, 3)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10ae3923-2970-4f8c-81fa-a822bfd5f214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([2, 2, 3])\n",
      "2\n",
      "torch.float32\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "## we can check the size and shape of the tensors\n",
    "print(zeros.size())\n",
    "print(ones.shape)\n",
    "## and we can simply access the values\n",
    "print(ones.shape[1])\n",
    "## we can also check the data type\n",
    "print(ones.dtype)\n",
    "## by default the type is float32\n",
    "## but we can change it when constructing the tensor\n",
    "tn = torch.rand(2, 3, dtype=torch.float16)\n",
    "print(tn.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38d08cc1-97bf-4677-8285-30aea0969b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int16)\n",
      "tensor([ 0.0000,  0.5791,  1.1582,  1.7373,  2.3164,  2.8945,  3.4746,  4.0508,\n",
      "         4.6328,  5.2109,  5.7891,  6.3672,  6.9492,  7.5273,  8.1016,  8.6875,\n",
      "         9.2656,  9.8438, 10.4219, 11.0000], dtype=torch.float16)\n",
      "tensor([ 0.0000,  0.5789,  1.1579,  1.7368,  2.3158,  2.8947,  3.4737,  4.0526,\n",
      "         4.6316,  5.2105,  5.7895,  6.3684,  6.9474,  7.5263,  8.1053,  8.6842,\n",
      "         9.2632,  9.8421, 10.4211, 11.0000], dtype=torch.float64)\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "0.0\n",
      "torch.Size([4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2069, 0.4138, 0.6206, 0.8276, 1.0342, 1.2412, 1.4482, 1.6553,\n",
       "        1.8623, 2.0684, 2.2754, 2.4824, 2.6895, 2.8965, 3.1035, 3.3105, 3.5176,\n",
       "        3.7246, 3.9316, 4.1367, 4.3438, 4.5508, 4.7578, 4.9648, 5.1719, 5.3789,\n",
       "        5.5859, 5.7930, 6.0000], dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can also create tensors from list or arrays\n",
    "lst = range(11)\n",
    "tnfls = torch.tensor(lst, dtype = torch.int16)\n",
    "print(tnfls)\n",
    "arr = np.linspace(0, 11, 20)\n",
    "## this will create a copy\n",
    "tnfar = torch.tensor(arr, dtype=torch.float16)\n",
    "print(tnfar)\n",
    "## where this way they'll share the same memory\n",
    "tnfarsm = torch.from_numpy(arr)\n",
    "print(tnfarsm)\n",
    "## slicing is similar to np arrays as well\n",
    "print(zeros[:, 1])\n",
    "print(zeros[0, :])\n",
    "## or access one item in specific index\n",
    "print(zeros[0,0, 0].item())\n",
    "## we can also reshape the tensor \n",
    "nten = tnfar.view(4, 5)\n",
    "print(nten.shape)\n",
    "## we can create a np array from a tensor\n",
    "## but we have to be carefull if we're using CPU\n",
    "## because they'll share the same memory loc\n",
    "## and change in one will change the other as well\n",
    "arrftn = tnfar.numpy()\n",
    "## we can aslo have pytorch calculating \n",
    "## the gradient for a tensor\n",
    "## which can be useful when we're optimizing \n",
    "arr2 = np.linspace(0, 6, 30)\n",
    "grtn = torch.tensor(arr2, dtype = torch.float16, requires_grad=True)\n",
    "grtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a439d3a0-1d52-4c54-a457-e0544af70472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4554, 0.7651],\n",
       "        [0.8806, 1.1813]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can also do all the operations \n",
    "## that we can do with np arrays with tensors\n",
    "rand1 = torch.rand(2, 2)\n",
    "rand2 = torch.rand(2, 2)\n",
    "## we can add them\n",
    "rand3 = torch.add(rand1, rand2)\n",
    "## or use inplace addition\n",
    "## rand1.add_(rand2)\n",
    "## sub\n",
    "rand4 = torch.sub(rand1, rand2)\n",
    "## multiply them\n",
    "rand5 = torch.mul(rand1, rand2)\n",
    "## divide them\n",
    "rand6 = torch.div(rand1, rand2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8642f63-7ac8-4117-8c5f-ca80f773179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7207, 0.3691],\n",
      "        [0.3076, 0.2598]], dtype=torch.float16)\n",
      "tensor([[0.3145, 0.5518, 0.7104],\n",
      "        [0.8188, 0.8291, 0.9517],\n",
      "        [0.8564, 0.5396, 0.5063]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "## by default all the tensors are created on CPU\n",
    "## but we can move them into GPU, or create them on GPU directly\n",
    "## we can create a device object\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "## and then pass it when we're creating a tensor\n",
    "## by moving them after creation\n",
    "nwtn = torch.rand(2, 2, dtype=torch.float16).to(device)\n",
    "## or at creation, which is more efficient\n",
    "nwtn2 = torch.rand(3, 3, dtype=torch.float16, device=device)\n",
    "print(nwtn)\n",
    "print(nwtn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfe0b5-bb8e-4c2e-9c78-de280231dbb2",
   "metadata": {},
   "source": [
    "<a id='autograd'></a>\n",
    "### Autograd\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on tensors; `torch.autograd` is an engine for computing the vector Jacobian product - it computes the partial derivates while applying the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80022364-14cc-4073-b716-19bb2cdd7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.3333, 0.6665, 1.0000, 1.3330, 1.6670, 2.0000, 2.3340, 2.6660,\n",
      "        3.0000], dtype=torch.float16, requires_grad=True)\n",
      "tensor([5.0000, 5.3320, 5.6680, 6.0000, 6.3320, 6.6680, 7.0000, 7.3359, 7.6641,\n",
      "        8.0000], dtype=torch.float16, grad_fn=<AddBackward0>)\n",
      "tensor([ 0.0000,  0.1899,  0.5049,  0.8076,  0.9707,  0.9229,  0.6855,  0.3579,\n",
      "         0.0864, -0.0205], dtype=torch.float16, grad_fn=<MulBackward0>)\n",
      "tensor(0.4504, dtype=torch.float16, grad_fn=<MeanBackward0>)\n",
      "None\n",
      "tensor([ 0.0284,  0.0815,  0.0999,  0.0753,  0.0188, -0.0463, -0.0911, -0.0969,\n",
      "        -0.0617,  0.0005], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "## we used the requires_grad = True in the section above\n",
    "## and now we can actually see the impact\n",
    "x = torch.linspace(0, 3, 10, requires_grad=True, dtype=torch.float16)\n",
    "y = x + 5\n",
    "print(x)\n",
    "## and we can see that now there's a grad_fn for y\n",
    "## and for this case is AddBackward\n",
    "## which uses the backpropagation\n",
    "print(y)\n",
    "## and we can see if we multiply the tensors\n",
    "## then the gard_fn will change to MulBackward\n",
    "z = torch.sin(x)*torch.cos(y)\n",
    "print(z)\n",
    "## and then to add more layer\n",
    "## if we get the mean then it'll become MeanBackward\n",
    "z = torch.mean(z)\n",
    "print(z)\n",
    "## and we can calculate the gradient \n",
    "## by calling the backward on the variable\n",
    "## and before calling it, there's no grad\n",
    "## for the variable that we're calculating\n",
    "## derivative with respect to\n",
    "print(x.grad)\n",
    "z.backward()\n",
    "## this will contain dz/dx values\n",
    "print(x.grad)\n",
    "## and we have to be carefull while using this \n",
    "## because each time we call backward on a function\n",
    "## the gradients accumulate and if we're looping \n",
    "## for our model training, we have to empty the grad attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc5de0e8-da1c-4627-89c8-2836919c8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<MulBackward0 object at 0x0000014C280871C0>\n",
      "False\n",
      "tensor([  5.0000,   6.2346,   9.9383,  16.1111,  24.7531,  35.8642,  49.4444,\n",
      "         65.4938,  84.0124, 105.0000])\n",
      "True\n",
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## there are cases where we don't want to track our history\n",
    "## during training when we want to update our weights\n",
    "## or when we're evaluating the model\n",
    "## to prevent the tracking\n",
    "## we can use .required_grad_(False)\n",
    "## or .detach()\n",
    "## or use the torch.no_grad() wrapper\n",
    "a = torch.linspace(0, 10, 10, requires_grad=True)\n",
    "b = a * 5\n",
    "print(a.requires_grad)\n",
    "print(b.grad_fn)\n",
    "## changing it inplace\n",
    "a.requires_grad_(False)\n",
    "c = a**2 + 5\n",
    "print(a.requires_grad)\n",
    "## and now c doesn't have a grad_fn\n",
    "print(c)\n",
    "## if we want to create a copy instead\n",
    "## we can use detach\n",
    "a.requires_grad_(True)\n",
    "d = a.detach()\n",
    "print(a.requires_grad)\n",
    "print(d.requires_grad)\n",
    "## and the 3rd way that's common for evaluation step\n",
    "with torch.no_grad():\n",
    "    e = a ** 2 + 5\n",
    "    print(e.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ef705-5c3a-47d1-8e6a-117333c9262e",
   "metadata": {},
   "source": [
    "#### Gradient Descent by Autograd\n",
    "\n",
    "reminder:\n",
    "$ f(x) =  \\omega \\times x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fba2c2ce-40a3-4d85-83c6-3fb516c2cc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated loss value 0.0, with w = 4.0, and for testing f(x=3) = 12.0\n"
     ]
    }
   ],
   "source": [
    "## lets create a simple example\n",
    "x = torch.arange(start=0, end=11)\n",
    "y = x * 4\n",
    "## create our initial valuse for w and b\n",
    "## and we need to set the requires_grad to True\n",
    "## and we will use a simple example with no intercept\n",
    "w = torch.zeros(1, requires_grad=True)\n",
    "## our main function will be \n",
    "def func(x, w):\n",
    "    return w * x\n",
    "## and then our loss is simply\n",
    "## mean squared errors for y values\n",
    "def loss(yhat, y):\n",
    "    return torch.mean((yhat-y)**2)\n",
    "## and then we can start with training\n",
    "## we also need a learning rate\n",
    "learning_rate = 0.01\n",
    "for _ in range(100):\n",
    "    ## first we calculate yhat\n",
    "    yhat = func(x, w)\n",
    "    ## and then the loss values\n",
    "    l = loss(yhat, y)\n",
    "    ## and all we need to do now is to call\n",
    "    ## backward on loss\n",
    "    l.backward()\n",
    "    ## and then we only have to update w\n",
    "    ## using the gradients\n",
    "    ## and we don't want them to accumulate\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad.data\n",
    "    ## and then after updating the w\n",
    "    ## we will empty the gradients \n",
    "    ## for the next loop\n",
    "    w.grad.data.zero_()\n",
    "    \n",
    "print(f'The calculated loss value {l}, with w = {w.item()}, and for testing f(x=3) = {func(3, w).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65cdf2-3ba0-44e5-a6ad-c34a90daf70f",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "#### Working with models, loss, and optimizers\n",
    "\n",
    "A PyTorch pipeline is usually like this:\n",
    "\n",
    "    1. Model Design - input, output, forward pass with multiple layers\n",
    "    2. Construct loss and optimizer\n",
    "    3. Training loop\n",
    "        - Forward - computing prediction and loss\n",
    "        - Backward - computing gradients\n",
    "        - Updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "94471659-833f-40e0-ab83-a3c987c756a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:1.9741042852401733, loss:0.0070242201909422874; f(x=3.0) = 6.102593898773193\n"
     ]
    }
   ],
   "source": [
    "## we will create our own model, by using nn module\n",
    "from torch import nn\n",
    "## and our class should inherit from nn\n",
    "class LinearRegression(nn.Module):\n",
    "    ## and we only need the input and output dimensions\n",
    "    def __init__(self, input_d, output_d):\n",
    "        ## we have to also use super\n",
    "        super(LinearRegression, self).__init__()\n",
    "        ## we have to then create the linear model\n",
    "        self.lin = nn.Linear(input_d,output_d)\n",
    "    ## the next step is to create our forward func\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "## we also need some sample data to use\n",
    "## which we want 10 samples for one featue\n",
    "X = torch.arange(1, 11, dtype=torch.float32).view(10, 1)\n",
    "y = 2 * X \n",
    "## then we have to create an instance of our model\n",
    "## by passing the # of features\n",
    "model = LinearRegression(X.shape[1], X.shape[1])\n",
    "## and then our loss function\n",
    "loss = nn.MSELoss()\n",
    "## and then our optimizer, which we will use \n",
    "## Stochastic Greadient Descent from pytorch\n",
    "## and pass the model parameters and the learning rate\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "## and we have to loop through and update our w\n",
    "for _ in range(200):\n",
    "    ## first we have to calculate our yhat\n",
    "    yhat = model(X)\n",
    "    ## and then calculating the loss\n",
    "    l = loss(y, yhat)\n",
    "    ## and then calculate the gradients\n",
    "    l.backward()\n",
    "    ## and then updating our params\n",
    "    optimizer.step()\n",
    "    ## and finally, zeroing out our \n",
    "    optimizer.zero_grad()\n",
    "## and at the end, we get the calculated values for w and b\n",
    "w, b = model.parameters()\n",
    "print(f'w:{w.item()}, loss:{l}; f(x=3.0) = {model(torch.tensor([3.0])).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b699b-4339-4e5b-bf68-8cc925505f89",
   "metadata": {},
   "source": [
    "<a id='fneuron'></a>\n",
    "\n",
    "### First Neural Net\n",
    "\n",
    "GPU, Datasets, DataLoader, Transformers, Neural Net, Training, and Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa2787-ccee-4d5e-a873-f9175b69afbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
